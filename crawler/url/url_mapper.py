"""
URL Mapper Utility for linking page_url.py with filtered content and RAG outputs.
Provides bidirectional mapping between original URLs and filtered filenames.
"""

import json
import re
from pathlib import Path
from typing import Dict, Optional
from urllib.parse import urlparse
import ast

from logger_config import get_logger

logger = get_logger(__name__)


class URLMapper:
    """
    URL Mapper that creates bidirectional mapping between original URLs from page_url.py
    and the filtered content filenames generated by the crawler.
    """

    def __init__(
        self,
        page_url_file: str,
        mapping_cache_file: Optional[str] = None,
        base_url: str = "",
    ):
        """
        Initialize URL mapper.

        Args:
            page_url_file: Path to page_url.py file containing original URLs
            mapping_cache_file: Path to cache file for storing mappings
            base_url: Base URL to prepend to relative URLs (e.g., "https://example.com").
                     If empty, will attempt to read base_url from page_url.py file.
        """
        self.page_url_file = page_url_file
        self.mapping_cache_file = (
            mapping_cache_file or "crawler/url/url_mapping_cache.json"
        )

        # If no base_url provided, try to read it from page_url.py file
        if not base_url:
            base_url = self._load_base_url_from_file()

        self.base_url = (
            base_url.rstrip("/") if base_url else ""
        )  # Remove trailing slash
        self.filename_to_url_map: Dict[str, str] = {}
        self.url_to_filename_map: Dict[str, str] = {}

        # Try to load existing cache
        self._load_mapping_cache()

    def url_to_filename(self, url: str) -> str:
        """
        Convert URL to filename using the same algorithm as crawler's get_safe_filename.

        Args:
            url: Original URL from page_url.py

        Returns:
            Safe filename that matches crawler's output
        """
        # Handle relative URLs (remove leading slash)
        if url.startswith("/"):
            path = url[1:]
        else:
            parsed = urlparse(url)
            path = parsed.path
            if path.startswith("/"):
                path = path[1:]

        # Apply crawler's transformation rules
        filename = path.replace("/", "_").replace("\\", "_")

        # Remove leading underscore if present
        if filename.startswith("_"):
            filename = filename[1:]

        # Handle empty filename
        if not filename or filename == "_":
            filename = "index"

        # Remove file extension if present (.htm or .html)
        if filename.endswith(".htm") or filename.endswith(".html"):
            filename = filename.rsplit(".", 1)[0]

        # Replace unsafe characters with underscores
        filename = re.sub(r'[<>:"/\\|?*]', "_", filename)

        # Limit length to 100 characters (as per crawler)
        return filename[:100]

    def get_relative_url(self, filename: str) -> str:
        """
        Get original relative URL from filename using cached mapping.

        Args:
            filename: Filtered content filename (without .html extension)

        Returns:
            Original relative URL from page_url.py, or empty string if not found
        """
        return self.filename_to_url_map.get(filename, "")

    def get_full_url(self, filename: str) -> str:
        """
        Get complete URL (base_url + relative_url) from filename using cached mapping.

        Args:
            filename: Filtered content filename (without .html extension)

        Returns:
            Complete URL with base_url prepended, or relative URL if no base_url set
        """
        relative_url = self.get_relative_url(filename)
        if not relative_url:
            return ""

        if self.base_url:
            return f"{self.base_url}{relative_url}"
        else:
            return relative_url

    def set_base_url(self, base_url: str) -> None:
        """
        Set or update the base URL.

        Args:
            base_url: Base URL to prepend to relative URLs (e.g., "https://example.com")
        """
        self.base_url = base_url.rstrip("/") if base_url else ""

    def build_mapping_cache(self) -> Dict[str, any]:
        """
        Build complete mapping from page_url.py and save to cache.

        Returns:
            Dictionary with mapping statistics and results
        """
        logger.info(f"Building URL mapping cache from {self.page_url_file}")

        try:
            # Load URLs from page_url.py
            urls = self._load_page_urls()

            if not urls:
                logger.warning("No URLs found in page_url.py")
                return {"success": False, "message": "No URLs found"}

            # Clear existing mappings
            self.filename_to_url_map.clear()
            self.url_to_filename_map.clear()

            # Build bidirectional mapping
            processed_count = 0
            duplicates = []

            for url in urls:
                filename = self.url_to_filename(url)

                # Check for filename collisions
                if filename in self.filename_to_url_map:
                    existing_url = self.filename_to_url_map[filename]
                    duplicates.append(
                        {
                            "filename": filename,
                            "existing_url": existing_url,
                            "new_url": url,
                        }
                    )
                    logger.warning(
                        f"Filename collision: {filename} maps to both {existing_url} and {url}"
                    )

                # Store mapping (later URL overwrites earlier one in case of collision)
                self.filename_to_url_map[filename] = url
                self.url_to_filename_map[url] = filename
                processed_count += 1

            # Save cache to file
            self._save_mapping_cache()

            result = {
                "success": True,
                "total_urls": len(urls),
                "processed_mappings": processed_count,
                "unique_filenames": len(self.filename_to_url_map),
                "duplicates": duplicates,
                "cache_file": self.mapping_cache_file,
            }

            logger.info(
                f"Mapping cache built successfully: {processed_count} mappings created"
            )
            if duplicates:
                logger.warning(f"Found {len(duplicates)} filename collisions")

            return result

        except Exception as e:
            logger.error(f"Error building mapping cache: {e}")
            return {"success": False, "error": str(e)}

    def _load_base_url_from_file(self) -> str:
        """Load base_url from page_url.py file."""
        try:
            with open(self.page_url_file, "r", encoding="utf-8") as f:
                content = f.read()

            # Parse the Python file content to extract base_url
            parsed_data = ast.literal_eval(content)
            base_url = parsed_data.get("base_url", "")

            if base_url:
                logger.info(f"Loaded base_url from {self.page_url_file}: {base_url}")
            else:
                logger.info(f"No base_url found in {self.page_url_file}")

            return base_url

        except Exception as e:
            logger.warning(f"Error loading base_url from file: {e}")
            return ""

    def _load_page_urls(self) -> list:
        """Load URLs from page_url.py file."""
        try:
            with open(self.page_url_file, "r", encoding="utf-8") as f:
                content = f.read()

            # Parse the Python file content to extract page_links
            parsed_data = ast.literal_eval(content)
            page_links = parsed_data.get("page_links", [])

            logger.info(f"Loaded {len(page_links)} URLs from {self.page_url_file}")
            return page_links

        except Exception as e:
            logger.error(f"Error loading page URLs: {e}")
            return []

    def _load_mapping_cache(self) -> bool:
        """Load existing mapping cache from file."""
        cache_path = Path(self.mapping_cache_file)

        if not cache_path.exists():
            logger.info(f"No existing cache found at {cache_path}")
            return False

        try:
            with open(cache_path, "r", encoding="utf-8") as f:
                cache_data = json.load(f)

            self.filename_to_url_map = cache_data.get("filename_to_url", {})
            self.url_to_filename_map = cache_data.get("url_to_filename", {})

            logger.info(
                f"Loaded mapping cache: {len(self.filename_to_url_map)} mappings"
            )
            return True

        except Exception as e:
            logger.error(f"Error loading mapping cache: {e}")
            return False

    def _save_mapping_cache(self) -> None:
        """Save current mapping to cache file."""
        cache_data = {
            "metadata": {
                "generated_at": self._get_current_timestamp(),
                "source_file": self.page_url_file,
                "total_mappings": len(self.filename_to_url_map),
                "description": "Bidirectional mapping between page URLs and filtered content filenames",
            },
            "filename_to_url": self.filename_to_url_map,
            "url_to_filename": self.url_to_filename_map,
        }

        # Ensure cache directory exists
        cache_path = Path(self.mapping_cache_file)
        cache_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)

            logger.info(f"Mapping cache saved to {cache_path}")

        except Exception as e:
            logger.error(f"Error saving mapping cache: {e}")

    def validate_mapping(self, filtered_content_dir: str) -> Dict[str, any]:
        """
        Validate mapping against actual filtered content files.

        Args:
            filtered_content_dir: Directory containing filtered HTML files

        Returns:
            Validation results with statistics and missing files
        """
        logger.info(f"Validating URL mapping against {filtered_content_dir}")

        filtered_path = Path(filtered_content_dir)
        if not filtered_path.exists():
            return {"success": False, "error": f"Directory not found: {filtered_path}"}

        # Get all HTML files in filtered content directory
        html_files = list(filtered_path.glob("*.html"))
        existing_filenames = {f.stem for f in html_files}  # Remove .html extension

        # Check which mappings have corresponding files
        mapped_filenames = set(self.filename_to_url_map.keys())

        found_files = mapped_filenames & existing_filenames
        missing_files = mapped_filenames - existing_filenames
        unmapped_files = existing_filenames - mapped_filenames

        validation_result = {
            "success": True,
            "total_mapped_urls": len(mapped_filenames),
            "total_filtered_files": len(existing_filenames),
            "found_files": len(found_files),
            "missing_files": list(missing_files),
            "unmapped_files": list(unmapped_files),
            "coverage_percentage": (
                (len(found_files) / len(mapped_filenames) * 100)
                if mapped_filenames
                else 0
            ),
        }

        logger.info(
            f"Validation complete: {len(found_files)}/{len(mapped_filenames)} mappings have corresponding files"
        )

        if missing_files:
            logger.warning(
                f"Missing files: {len(missing_files)} mapped URLs have no corresponding filtered content"
            )

        if unmapped_files:
            logger.info(
                f"Unmapped files: {len(unmapped_files)} filtered files have no URL mapping"
            )

        return validation_result

    def get_mapping_stats(self) -> Dict[str, any]:
        """Get statistics about current mapping."""
        return {
            "total_mappings": len(self.filename_to_url_map),
            "cache_file": self.mapping_cache_file,
            "cache_exists": Path(self.mapping_cache_file).exists(),
            "sample_mappings": dict(
                list(self.filename_to_url_map.items())[:5]
            ),  # First 5 for preview
        }

    def _get_current_timestamp(self) -> str:
        """Get current timestamp in ISO format."""
        from datetime import datetime

        return datetime.now().isoformat()


# Convenience functions for easy usage
def create_url_mapper(page_url_file: str = "crawler/url/page_url.py") -> URLMapper:
    """Create URL mapper with default settings."""
    return URLMapper(page_url_file)


def build_mapping_cache(
    page_url_file: str = "crawler/url/page_url.py",
) -> Dict[str, any]:
    """Build mapping cache with default settings."""
    mapper = create_url_mapper(page_url_file)
    return mapper.build_mapping_cache()


def validate_url_mapping(
    filtered_content_dir: str = "crawler/result_data/filtered_content",
) -> Dict[str, any]:
    """Validate URL mapping against filtered content directory."""
    mapper = create_url_mapper()
    return mapper.validate_mapping(filtered_content_dir)


if __name__ == "__main__":
    # CLI interface for testing
    import argparse

    parser = argparse.ArgumentParser(description="URL Mapper Utility")
    parser.add_argument(
        "--build-cache", action="store_true", help="Build mapping cache"
    )
    parser.add_argument("--validate", action="store_true", help="Validate mapping")
    parser.add_argument("--stats", action="store_true", help="Show mapping statistics")
    parser.add_argument(
        "--page-url-file", default="crawler/url/page_url.py", help="Page URL file"
    )
    parser.add_argument(
        "--filtered-dir",
        default="crawler/result_data/filtered_content",
        help="Filtered content directory",
    )

    args = parser.parse_args()

    mapper = URLMapper(args.page_url_file)

    if args.build_cache:
        result = mapper.build_mapping_cache()
        print(f"Cache build result: {result}")

    if args.validate:
        result = mapper.validate_mapping(args.filtered_dir)
        print(f"Validation result: {result}")

    if args.stats:
        stats = mapper.get_mapping_stats()
        print(f"Mapping statistics: {stats}")
